{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 3\n",
    "\n",
    "# Diego Estuar, Yannick Angouo Lopes, Andrew Xu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load house price dataset (training set) from github\n",
    "df = pd.read_csv(\"https://github.com/ArinB/MSBA-CA-Data/raw/main/CA01/house-price-train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the DataFrame's shape\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the DataFrame's data types\n",
    "pd.set_option('display.max_rows', None)\n",
    "#pd.reset_option('max_rows')\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding missing values\n",
    "missing_values = df.isnull().sum()\n",
    "# Get columns with missing values\n",
    "columns_with_missing_values = missing_values[missing_values > 0].index\n",
    "print(columns_with_missing_values)\n",
    "# Print missing features and it's number of missing values\n",
    "df[columns_with_missing_values].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create visulizations for df, if it's a number, create a histogram, if it's an object/categoricable variable, create a bar plot with counts of each categories.\n",
    "# Determine the type of each column\n",
    "# List of specific columns to treat as categorical which are numbers in the dataset\n",
    "categorical_cols = ['MSSubClass', 'OverallQual', 'OverallCond']\n",
    "\n",
    "for column in df.columns:\n",
    "    if column != 'Id':\n",
    "        if df[column].dtype == 'object' or column in categorical_cols:\n",
    "            # It's a categorical column, create a bar plot\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            sns.countplot(x=column, data=df)\n",
    "            plt.title(f'Bar Plot of {column}')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "        else:\n",
    "            # It's a numeric column, create a histogram\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.hist(df[column], bins=10, alpha=0.7)\n",
    "            plt.title(f'Histogram of {column}')\n",
    "            plt.xlabel(column)\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the barplot of house type we can see that the most common house type is one-story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate basic statistics for a numeric column\n",
    "pd.set_option('display.max_rows', None)\n",
    "for column in df.columns:\n",
    "    if column != 'Id':\n",
    "        if df[column].dtype != 'object' and column not in categorical_cols:\n",
    "            print(df[column].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the correlation between two numeric columns\n",
    "#fireplaces and sales price\n",
    "print(df['Fireplaces'].corr(df['SalePrice']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sales price group by pool quality \n",
    "grouped_df_1 = df.groupby('PoolQC')['SalePrice'].mean()\n",
    "print(grouped_df_1)\n",
    "#sales price group by fence quality \n",
    "grouped_df_2 = df.groupby('Fence')['SalePrice'].mean()\n",
    "print(grouped_df_2)\n",
    "#From here we learn that there are no TA(average)/NA(No) pool type houses in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a scatter plot to visualize the relationship between two numeric columns\n",
    "plt.scatter(df['GrLivArea'], df['SalePrice'])\n",
    "plt.xlabel('GrLivArea')\n",
    "plt.ylabel('SaleP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The insight gained from this analysis is that, in this dataset, \n",
    "#the Above grade (ground) living area square feet, as measured by 'GrLivArea', is a significant factor influencing the sale price. \n",
    "#Buyers are willing to pay more for houses with more above grade (ground) living area square feet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yannick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are interested in understanding how the overall quality of a house ('OverallQual') \n",
    "#is related to its sale price ('SalePrice'). \n",
    "#We want to determine whether houses with higher overall quality tend to have higher sale prices.\n",
    "#Here's the thought process and steps to analyze this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1: group the data by a categorical column and calculate statistics\n",
    "grouped_df = df.groupby('OverallQual')['SalePrice'].mean()\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a scatter plot to visualize the relationship between two numeric columns\n",
    "plt.scatter(df['OverallQual'], df['SalePrice'])\n",
    "plt.xlabel('OverallQ')\n",
    "plt.ylabel('SaleP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The insight gained from this analysis is that, in this dataset, \n",
    "#the quality of a house, as measured by 'OverallQual', is a significant factor influencing the sale price. \n",
    "#Buyers are willing to pay more for houses with better overall quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a box plot to visualize the distribution of a numeric column\n",
    "plt.boxplot(df['SalePrice'])\n",
    "plt.ylabel('SaleP')\n",
    "plt.show() \n",
    "#The outliers hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bar plot to visualize the mean of a numeric column for each category of a categorical column\n",
    "#SalePrice by Neighborhood\n",
    "df_grouped = df.groupby('Neighborhood')\n",
    "df_grouped = df_grouped['SalePrice'].mean()\n",
    "df_grouped.plot(kind='bar')\n",
    "plt.ylabel('Mean SalePrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a pivot table to summarize the data\n",
    "## Question: \"What is the mean sale price of houses across different neighborhoods categorized by their overall quality?\"\n",
    "pivot_table = df.pivot_table(index='Neighborhood', columns='OverallQual', values='SalePrice', aggfunc='mean')\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it differentiates the price impact of both the property's intrinsic quality and its location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a heatmap to visualize the pivot table\n",
    "plt.pcolor(pivot_table, cmap='Reds')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Overall Quality')\n",
    "plt.ylabel('Neighborhood')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pairplot to visualize the relationships between multiple numeric columns\n",
    "sns.pairplot(df, vars=['YearBuilt', 'TotalBsmtSF', 'SalePrice']) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sale prices has a stronger positive relationship with total basement square feet than with year built, \n",
    "# and the size of basement square feet didn't increase much over years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a bar plot to visualize the count of a categorical column\n",
    "df['OverallQual'].value_counts().plot(kind='bar')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a countplot to visualize the count of a categorical column by the categories of another categorical column\n",
    "plt.figure(figsize=(15, 4))\n",
    "sns.countplot(x='OverallQual', hue='GarageFinish', data=df)\n",
    "plt.legend(loc='upper right') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we can see most houses that have a quality score lower than 6 has unfinished garages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yannick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a violin plot to visualize the distribution of a numeric column by the categories of a categorical column\n",
    "sns.violinplot(x='HouseStyle', y='SalePrice', data=df)\n",
    "plt.ylabel('SalePrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a box plot to visualize the distribution of a numeric column by the categories of a categorical column\n",
    "sns.boxplot(x='HouseStyle', y='SalePrice', data=df)\n",
    "plt.ylabel('SalePrice')\n",
    "plt.show() \n",
    "#we can see a lot of outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a swarm plot to visualize the distribution of a numeric column by the categories of a categorical column\n",
    "# Fine tune the size of the markers so we won't lose any datapoints.\n",
    "sns.swarmplot(x='HouseStyle', y='SalePrice', data=df, size = 1.2)\n",
    "plt.ylabel('SalePrice')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "g = sns.FacetGrid(df, col='HouseStyle')\n",
    "g.map(plt.hist, 'SalePrice')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a heatmap to visualize the correlation between multiple columns\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df[['SalePrice', 'OverallQual', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt', 'GarageCars', 'GarageArea']].corr(), cmap='RdYlGn', annot=True)\n",
    "plt.show() \n",
    "\n",
    "#This heatmap will give you a visual representation of how these variables correlate with each other, \n",
    "#especially with 'SalePrice',which could provide valuable insights into which factors are most strongly associated \n",
    "#with the price at which a house sells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a scatter plot matrix to visualize the relationships between multiple numeric columns\n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(df[['SalePrice', 'OverallQual', 'GrLivArea']], alpha=0.2, figsize=(10, 10))\n",
    "plt.show() \n",
    "#This will create a matrix of scatter plots that can help you visualize the relationships between the sale price \n",
    "#of a property, its overall quality, and its living area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a regression plot to visualize the relationship between two numeric columns\n",
    "#sns.regplot(x='age', y='fare', data=df)\n",
    "#plt.show() \n",
    "\n",
    "sns.regplot(x='GrLivArea', y='SalePrice', data=df)\n",
    "plt.show() \n",
    "#This plot will help in understanding if larger houses (in terms of living area) \n",
    "#tend to sell for higher prices, and the strength and tightness of the scatter \n",
    "#around the regression line will also give insight into the variability of sale prices relative to living area size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a barplot to visualize the mean of a numeric column by the categories of a categorical column\n",
    "plt.figure(figsize=(15,4))\n",
    "sns.barplot(x='Neighborhood', y='SalePrice', data=df)\n",
    "plt.ylabel('Average Sale Price')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "#choose 'Neighborhood' as the categorical variable to see how the average sale price varies by neighborhood, \n",
    "#which could reveal location-based value differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Andrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a lmplot to visualize the relationship between two numeric columns and the categories of a categorical column\n",
    "sns.lmplot(x='GrLivArea', y='SalePrice', hue='Neighborhood', data=df, height=10, aspect=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With neighborhood as category, we can see more trends between above grade (ground) living area square feet and sales price in details. \n",
    "# Some neighborhoods have a stronger positive relationships compared to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a categoryplot to visualize the distribution of a numeric column by the categories of a categorical column\n",
    "sns.catplot(x='Neighborhood', y='SalePrice', data=df, height=10, aspect=1)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel('Average Sales Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NorthRidge has the most expensive house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a boxenplot to visualize the distribution of a numeric column by the categories of a categorical column\n",
    "plt.figure(figsize = (10, 10))\n",
    "sns.boxenplot(x='Neighborhood', y='SalePrice', data=df)\n",
    "plt.ylabel('Age')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a distplot to visualize the distribution of a numeric column\n",
    "sns.distplot(df['SalePrice'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a kdeplot to visualize the kernel density estimate of a numeric column\n",
    "sns.kdeplot(df['SalePrice'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Report\n",
    "These following features has missing values:\n",
    "\n",
    "'LotFrontage': median impute\n",
    "\n",
    "'Alley': delete\n",
    "\n",
    "'MasVnrType': Hard to impute becasue it's a categorical variable, perhaps we can impute the value according to MasVrnArea, or just delete the rows with the missing values\n",
    "\n",
    "'MasVnrArea': median impute\n",
    "\n",
    "'BsmtQual': Hard to impute because it's a categorical variable, since the missing amount is small, maybe we can delete the rows that contains the missing values.\n",
    "\n",
    "'BsmtCond': Hard to impute because it's a categorical variable, since the missing amount is small, maybe we can delete the rows that contains the missing values.\n",
    "\n",
    "'BsmtExposure': Hard to impute because it's a categorical variable, since the missing amount is small, maybe we can delete the rows that contains the missing values.\n",
    "\n",
    "'BsmtFinType1': Hard to impute because it's a categorical variable, since the missing amount is small, maybe we can delete the rows that contains the missing values.\n",
    "\n",
    "'BsmtFinType2': Hard to impute because it's a categorical variable, since the missing amount is small, maybe we can delete the rows that contains the missing values.\n",
    "\n",
    "'Electrical': Hard to impute because it's a categorical variable, since the missing amount is small, maybe we can delete the rows that contains the missing values.\n",
    "\n",
    "'FireplaceQu': Hard to impute because it's a categorical variable, we can delete the column.\n",
    "\n",
    "'GarageType': Hard to impute because it's a categorical variable, since the missing amount is small, maybe we can delete the rows that contains the missing values.\n",
    "\n",
    "'GarageYrBlt': Hard to impute because it's a categorical variable, since the missing amount is small, maybe we can delete the rows that contains the missing values.\n",
    "\n",
    "'GarageFinish': Hard to impute because it's a categorical variable, since the missing amount is small, maybe we can delete the rows that contains the missing values. \n",
    "\n",
    "'GarageQual': Hard to impute because it's a categorical variable, since the missing amount is small, maybe we can delete the rows that contains the missing values.\n",
    "\n",
    "'GarageCond': Hard to impute because it's a categorical variable, since the missing amount is small, maybe we can delete the rows that contains the missing values. \n",
    "\n",
    "'PoolQC': delete\n",
    "\n",
    "'Fence': delete\n",
    "\n",
    "'MiscFeature': delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the original DataFrame\n",
    "df_preprocessed = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall features with missing values\n",
    "# Finding missing values\n",
    "missing_values = df.isnull().sum()\n",
    "# Get columns with missing values\n",
    "columns_with_missing_values = missing_values[missing_values > 0].index\n",
    "print(columns_with_missing_values)\n",
    "# Print missing features and it's number of missing values\n",
    "df[columns_with_missing_values].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in the DataFrame\n",
    "\n",
    "# This technique is used when there are missing values in the dataset. \n",
    "# There are various ways to handle missing values, such as filling them with the mean, \n",
    "# median, or mode of the column, or dropping rows with missing values. The appropriate \n",
    "# method will depend on the specific dataset and the goal of the analysis.\n",
    "\n",
    "# Impute missing values with 'median'\n",
    "\n",
    "# Columns with more than 60% of missing values\n",
    "threshold = 1460 * 0.6\n",
    "columns_to_keep = missing_values[missing_values <= threshold].index\n",
    "\n",
    "# Keep only the ones with <= 60% of missing values\n",
    "df_new = df[columns_to_keep].copy()\n",
    "\n",
    "# Impute the remaining missing values with medians correspondingly\n",
    "df_new['MasVnrArea'].fillna(df_new['MasVnrArea'].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete rows with missing values\n",
    "df_clean = df_new.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate summary statistics for a numeric column\n",
    "print(df_clean['SalePrice'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the skewness and kurtosis of a numeric column\n",
    "print(df_clean['SalePrice'].skew())\n",
    "print(df_clean['SalePrice'].kurtosis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correlation between two numeric columns\n",
    "print(df_clean['SalePrice'].corr(df_clean['GrLivArea']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a t-test to compare the means of two numeric columns\n",
    "%pip install scipy\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "t, p = ttest_ind(df_clean['SalePrice'], (df_clean['GrLivArea']))\n",
    "print(t, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform an ANOVA test to compare the means of two or more numeric columns\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "f, p = f_oneway(df_clean['SalePrice'], (df_clean['GrLivArea']))\n",
    "print(f, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_new.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROP COLUMNS THAT DOES NOT HAVE PREDICTIVE VALUES - Use Business Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Ticket Number and Cabin Number columns\n",
    "df_select_columns = df_clean.drop(columns = ['Street', 'LotFrontage'])\n",
    "print(df_select_columns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE CATEGORICAL VARIABLES TO NUMERICAL DUMMY COLUMNS -- mandatory before using ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables using one-hot encoding. All Columns must have numerical values after this step\n",
    "categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns\n",
    "df_encoded = pd.get_dummies(df_clean, columns=categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if all variables are numbers\n",
    "print(df_encoded.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the values of a numeric column\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_encoded['SalePrice_scaled'] = scaler.fit_transform(df_encoded[['SalePrice']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_encoded['SalePrice_scaled'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin the values of a numeric column\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=8, encode='ordinal')\n",
    "df_encoded['SalePrice_binned'] = discretizer.fit_transform(df_encoded[['SalePrice']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a min-max scaling to a numeric column\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_encoded['SalePrice_scaled_minmax'] = scaler.fit_transform(df_encoded[['SalePrice']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a robust scaling to a numeric column\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "df_encoded['SalePrice_robust'] = scaler.fit_transform(df_encoded[['SalePrice']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a power transformation to a numeric column\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "transformer = PowerTransformer(method='yeo-johnson')\n",
    "df_encoded['SalePrice_PowerTransformed'] = transformer.fit_transform(df_encoded[['SalePrice']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a box-cox transformation to a numeric column\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "df_encoded['SalePrice_boxcox'], lambda_ = boxcox(df_encoded['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEP ONLY REQUIRED COLUMNS - Let's just use the Binned Values for SalePrice DROP other processed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_encoded = df_encoded.drop(columns=['SalePrice','SalePrice_scaled_minmax','SalePrice_robust','SalePrice_PowerTransformed','SalePrice_boxcox'])\n",
    "#df_encoded = df_encoded.drop(columns= ['SalePrice_powerTransformed', 'SalePrice_scale_minmax'])\n",
    "#df_encoded = df_encoded.drop(columns= ['SalePrice_scaled'])\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the y (dependent variable) and X (independent variables)\n",
    "\n",
    "X = df_encoded.drop(['SalePrice_binned'], axis = 1)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_encoded['SalePrice_binned']\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show correlation between all variables\n",
    "corr_matrix = df_encoded.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIF method\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X = df_encoded.copy()\n",
    "X['Intercept'] = 1\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Variable'] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairplot for multicollinearity\n",
    "sns.pairplot(df_encoded) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIF iteration\n",
    "def calculate_vifs(df):\n",
    "    X = df.copy()\n",
    "    X['Intercept'] = 1 # add an intercept\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Variable'] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "def remove_high_vif_columns(df, threshold = 10.0):\n",
    "    while True:\n",
    "        vifs = calculate_vifs(df)\n",
    "        max_vif = vifs['VIF'].max()\n",
    "        if max_vif > threshold:\n",
    "            max_vif_var = vifs.sort_values('VIF', ascending = False)['Variable'].iloc[0]\n",
    "            if max_vif_var == 'Intercept':\n",
    "                break #don't remove the intercept\n",
    "            print(f\"Removing '{max_vif_var}' with VIF: {max_vif}\")\n",
    "            df = df.drop(columns = [max_vif_var])\n",
    "        else:\n",
    "            break\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the functions\n",
    "#we use VIF method to determine which variables should be removed based on their multicollinearities with the threshold setting to 10.\n",
    "df_encoded_cleaned = remove_high_vif_columns(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The remaining variables after dropping the variables with VIF score > 10.\n",
    "df_encoded_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
